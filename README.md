#  Recriando o Modulo Inventory do Data Warehouse AdventureWork 2022. 

ğŸ“Œ 1. Objetivo do projeto

Construir um Data Warehouse para o mÃ³dulo Inventory do AdventureWorks 2022, replicando o fluxo OLTP â†’ DW para permitir anÃ¡lises histÃ³ricas e mÃ©tricas de inventÃ¡rio de modo simplificado.

Este projeto foi desenvolvido com foco em:
- Criar e carregar tabelas de staging (stg_)
- Criar e carregar tabelas de DimensÃ£o e Fato
- Aplicar transformaÃ§Ãµes simples para padronizaÃ§Ã£o
- Utilizar MERGE para cargas
- ImplementaÃ§Ã£o de chaves surrogate (SK)
- PadronizaÃ§Ã£o de datas via DimDate

---

ğŸ“Œ 2. Ferramentas Utilizadas

- **SQL Server**
- **SQL Server Management Studio (SSMS)**
- **AdventureWorks 2022 OLTP**


---

ğŸ“Œ 3. Arquitetura do Projeto

O DW foi construÃ­do seguindo um modelo dimensional no formato Star Schema, porÃ©m com caracterÃ­sticas de Snowflake tambÃ©m, por conta da hierarquia de produtos.

Tabelas de DimensÃ£o:
- DimProduct
- DimProductSubcategory
- DimProductCategory
- DimDate

Tabela de Fato:
- FactProductInventory

flowchart LR
    FactProductInventory --> DimProduct
    FactProductInventory --> DimDate
    DimProduct --> DimProductSubcategory
    DimProductSubcategory --> DimProductCategory
    
---


ğŸ“Œ 4. Etapas

Processo de ETL
O processo de carga do DW foi estruturado seguindo uma sequÃªncia lÃ³gica e simples, priorizando clareza e organizaÃ§Ã£o:

4.1 Estudo da CriaÃ§Ã£o da tabela DimDate

A primeira estrutura criada foi a DimDate, responsÃ¡vel por padronizar todas as datas usadas no DW.
Ela foi populada inicialmente com todo o calendÃ¡rio necessÃ¡rio para suportar a Fact.

4.2 CriaÃ§Ã£o das tabelas de Staging (STG)

Foram criadas tabelas intermediÃ¡rias (staging) para as DimensÃµes para receber os dados brutos vindos do OLTP.
Essas tabelas serviram como camada temporÃ¡ria para armazenar, padronizar e validar os dados antes de carregÃ¡-los no DW.
Durante a etapa de preparaÃ§Ã£o das tabelas STG, foi necessÃ¡rio identificar onde cada informaÃ§Ã£o estava armazenada no banco OLTP.
Realizar joins entre as tabelas do OLTP para reconstruir informaÃ§Ãµes que estavam normalizadas para uso no DW.


4.3 CriaÃ§Ã£o das tabelas finais do Data Warehouse

ApÃ³s a camada STG, foram criadas as tabelas dimensionais finais.
Essas tabelas foram preparadas com suas surrogate keys, tipos de dados definitivos e estrutura final do Schema.

4.4 TransformaÃ§Ã£o e carga das tabelas STG

Os dados foram carregados na camada STG utilizando processos de transform/load, incluindo:

- Ajuste de tipos
- SeleÃ§Ã£o das colunas relevantes
- PadronizaÃ§Ã£o de dados
- PreparaÃ§Ã£o para integridade referencial

4.5 Carga das DimensÃµes usando MERGE

Com as tabelas STG prontas, para realmente simular um DW sendo atualizado com novos dados, as dimensÃµes finais foram alimentadas usando comandos MERGE, permitindo:

- Inserir registros novos
- Evitar duplicidades
- Sincronizar as dimensÃµes com as tabelas de origem

4.6 CriaÃ§Ã£o e carga da tabela de fato (FactProductInventory)

Diferente das dimensÃµes, a Fact foi criada sem passar pela camada STG, para simplificar o processo.
Ela foi carregada diretamente com um INSERT + JOIN entre:

- DimProduct
- DimDate


4.7 CriaÃ§Ã£o dos relacionamentos (Star Schema)

Por fim, foram criados os relacionamentos entre:

- FactProductInventory â†’ DimProduct
- FactProductInventory â†’ DimDate
- DimProduct â†’ DimProductSubcategory
- DimProductSubcategory â†’ DimProductCategory

---


ğŸ“Œ 5 Regras de NegÃ³cio

- Cada movimento pertence a um produto (ProductKey)
- Cada movimento pertence a um dia especÃ­fico (DateKey)
- Integridade garantida via FK (ProductKey, DateKey)

---


ğŸ“Œ 6 DecisÃµes de Modelagem

- Colunas de traduÃ§Ã£o (Spanish, French, etc) como NULL nas dimensÃµes: para reduzir volume e manter o foco apenas nos atributos necessÃ¡rios ao entendimento do fluxo ETL.
- Colunas NULL na Fact: refletindo ausÃªncia de dados completos no OLTP e para simplificar a modelagem, mas mantendo consistÃªncia com o propÃ³sito do projeto.


ğŸ“Œ 7 ValidaÃ§Ãµes Realizadas

- ConferÃªncia de granularidade da Fact
- VerificaÃ§Ã£o de schema via `sp_help`
- Testes de integridade referencial (FKs)
- ConferÃªncia de duplicidades
- ConfirmaÃ§Ã£o de SKs funcionando

---

ğŸ“Œ 8 Estrutura do RepositÃ³rio

sql/
â”£ create_tables/
â”ƒ   â”— create_tables.sql
â”ƒ   â”— create_tables_stg.sql

â”£ load/                      
â”ƒ   â”£ load_stg_dim_product.sql
â”ƒ   â”£ load_stg_dim_product_category.sql
â”ƒ   â”£ load_stg_dim_product_sub_category.sql
â”ƒ   â”£ load_dim_date.sql
â”ƒ   â”— load_fact_product_inventory.sql

â”£ merge/                  
â”ƒ   â”£ merge_dim_product.sql
â”ƒ   â”£ merge_dim_product_category.sql
â”ƒ   â”— merge_dim_product_sub_category.sql


---

ğŸ“Œ 9 Aprendizados

Durante o desenvolvimento deste DW, aprendi e pratiquei:

- Modelagem dimensional com linguagem SQL no SQL Server.
- Identificar e mapear dados no OLTP para modelar as tabelas do DW (OLAP).
- CriaÃ§Ã£o de dimensÃµes e fato, alÃ©m do uso de chaves surrogate (IDENTITY).
- ConstruÃ§Ã£o e padronizaÃ§Ã£o de DimDate.
- Processo ETL completo: extraÃ§Ã£o, transformaÃ§Ã£o e carga.
- Como criar tabelas STG e usÃ¡-las como camada intermediÃ¡ria.
- Uso do comando MERGE para atualizar/insert na camada DW.
- Boas prÃ¡ticas de documentaÃ§Ã£o para projetos de dados.


---

ğŸ“Œ 10 PrÃ³ximos Passos

- Adicionar mais mÃ³dulos do AdventureWorks


ğŸ“Œ 11 ReferÃªncias

 https://dataedo.com/samples/html/Data_warehouse/


---

ğŸ“Œ 12 Autor
**Leonardo Bonani** 
 Contato: *www.linkedin.com/in/leonardo-bonani - leonardo_bonani@hotmail.com **  

---

